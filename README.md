# Analyzing the Impact of RAG in LLM-Based Code Generation

This repository contains source code and experimental scripts related to the exploration and implementation of prompting techniques for large language models (LLMs), with a focus on self-evaluation methods.

**Authors:** [Paraskevi Kivroglou]¹, [Prof. Dr. Martin, Simon]¹, [Prof. Dr-Ing. Schlippe, Tim,]¹

**Affiliations:** ¹IU Applied Sciences  

**Paper Type:** Master's Thesis

## Project Structure

thesis-repo/
├── EvaluationScripts/        # Main evaluation and testing scripts
├── Helpers/                  # Utility functions and helper modules
├── Scenario1a&b/            # First experimental scenario implementations
├── Scenario2a&b/            # Second experimental scenario implementations  
├── Scenario3a/              # Third experimental scenario (part a)
├── Scenario3b/              # Third experimental scenario (part b)
├── Scenario4/               # Fourth experimental scenario
├── WriteResults/            # Results output and analysis scripts
├── install_pciutils.sh      # Installation script for dependencies
├── install_requirements.py  # Python requirements installer
├── requirements.txt         # Project dependencies
├── test_setup_and_run.py   # Main test execution script
├── LICENSE                  # Project license
└── README.md               # Project documentation

## Contact and Collaboration

Provide contact information for follow-up research:

Email: [paraskevikivroglou@gmail.com]

Supervisor: [Prof. Dr. Martin, Simon] ([simon.martin@iu.org])




